{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e06c7ae",
   "metadata": {},
   "source": [
    "# CHUNKING STRATEGY\n",
    "RULES OF CHUNKING - \n",
    "1. Process every document separately.\n",
    "2. Remove all elements before 'PART I' Business Strategy, 'Table of Contents', and 'Page Numbers'.\n",
    "3. Chunk all lowest-level headings. Implement a 20% overlap.\n",
    "4. Merge two chunks if one has less than 200 tokens.\n",
    "5. Add the full hierarchical path of all headings/subheadings and document metadata to the metadata tag.\n",
    "(e.g., document_id, fiscal_year, part, section_1, section_2, section_3)\n",
    "6. Send the chunks along with metadata information to the LLM.\n",
    "7. Architectural Layer: Implement a Parent-Child Retrieval mechanism where search is done on the fine-grained chunks, but chunks under second-level heading will be combined and sent to LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfe317a",
   "metadata": {},
   "source": [
    "## 10-K CHUNKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe6002a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "126c9405",
   "metadata": {},
   "source": [
    "### Algorithm - \n",
    "1. Iterate through every item one by one.\n",
    "2. Initialize a Dynamic meta data dictionary for every chunk with a 'parent_header' : '{item_no: text}', 'form_type':'10_K', 'Company':{company name}, 'filing_date':'{date}'\n",
    "3. If any element is a header, mention '--- heading' in front of that and join it back with the parent chunk.\n",
    "4. Every chunk should contain maximum 250 words.\n",
    "5. Mention table in the METADATA of table text and keep it a standalone text, no matter what the size is.\n",
    "\n",
    "### Cleaning Chunks -\n",
    "1. Remove the footers using this pattern '.* |.* |[0-9]*'\n",
    "2. Remove the text if they only contain a single number '[0-9]+', it will remove page numbers.\n",
    "3. Remove any text which contains '[a-z]* inc.' and has less than 3 words, lowercase the text for this.\n",
    "\n",
    "### Further Refinements (Semantic Chunking) - \n",
    "1. For every Heading.\n",
    "2. Check if the dictionary of headings is empty, if yes append level 1 heading to the dict.\n",
    "3. Else check for the max heading num, then append it as max + 1.\n",
    "4. Check if previous chunk was content and has atleast 10 words, then empty the heading dict and append a new heading in level '1' also append the chunk as a new one and empty curr_chunk.\n",
    "5. While appending content check if the chunk size is 200 words, append if yes. Else continue adding.\n",
    "6. If consecutively more than 3 headings occur, continue till the next content appears, append all headings as content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315d24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from edgar import set_identity, Filing, find, use_local_storage, set_local_storage_path\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from dotenv import loadenv\n",
    "import math\n",
    "from edgar.files.html_documents import TextBlock, TableBlock\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f430e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_COUNTER = 1\n",
    "TENK_TENQ_CHUNKS = []\n",
    "TENK_CHUNKS = []\n",
    "TENQ_CHUNKS = []\n",
    "TOTAL_CHUNKS_TENK = 0\n",
    "TOTAL_CHUNKS_TENQ = 0\n",
    "#Reading all the metadata from 10-K\n",
    "BASE_PATH_TENK = 'path_to tenk_metadata'\n",
    "BASE_PATH_TENQ = 'path_to tenq_metadata'\n",
    "\n",
    "loadenv()\n",
    "set_identity(os.getenv(\"SEC_ID\"))\n",
    "use_local_storage(os.getenv(\"SEC_CACHE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a65ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paymo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\quantigence-oNV_nXK--py3.11\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AAPL, 2025-10-31\n",
      "Processed AAPL, 2024-11-01\n",
      "Processed MSFT, 2025-07-30\n",
      "Processed MSFT, 2024-07-30\n",
      "Processed GOOGL, 2025-02-05\n",
      "Processed GOOGL, 2024-01-31\n",
      "Processed AMZN, 2025-02-07\n",
      "Processed AMZN, 2024-02-02\n",
      "Processed META, 2025-01-30\n",
      "Processed META, 2024-02-02\n",
      "Processed NVDA, 2025-02-26\n",
      "Processed NVDA, 2024-02-21\n",
      "Processed TSLA, 2025-04-30\n",
      "Processed TSLA, 2025-01-30\n",
      "Processed TSLA, 2024-01-29\n",
      "Processed ORCL, 2025-06-18\n",
      "Processed ORCL, 2024-06-20\n",
      "Processed CRM, 2025-03-05\n",
      "Processed CRM, 2024-03-06\n",
      "Processed NFLX, 2025-01-27\n",
      "Processed NFLX, 2024-01-26\n",
      "Processed ADBE, 2025-01-13\n",
      "Processed ADBE, 2024-01-17\n"
     ]
    }
   ],
   "source": [
    "\n",
    "COMPANY_NAMES = [\n",
    "        \"Apple Inc.\",\n",
    "        \"MICROSOFT CORP\",\n",
    "        \"Alphabet Inc.\",\n",
    "        \"AMAZON COM INC.\",\n",
    "        \"Meta Platforms, Inc.\",\n",
    "        \"NVIDIA CORP\",\n",
    "        \"Tesla, Inc.\",\n",
    "        \"ORACLE CORP\",\n",
    "        \"Salesforce, Inc.\",\n",
    "        \"NETFLIX INC\",\n",
    "        \"ADOBE INC.\"\n",
    "    ]\n",
    "\n",
    "TICKERS = {\n",
    "        320193 : \"AAPL\",\n",
    "        789019: \"MSFT\",\n",
    "        1652044: \"GOOGL\",\n",
    "        1018724: \"AMZN\",\n",
    "        1326801 : \"META\",\n",
    "        1045810: \"NVDA\",\n",
    "        1318605 : \"TSLA\",\n",
    "        1341439 : \"ORCL\",\n",
    "        1108524 : \"CRM\",\n",
    "        1065280 : \"NFLX\",\n",
    "        796343 : \"ADBE\"\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "#Post processing the exisiting chunks\n",
    "def post_processing_chunks(all_chunks):\n",
    "    global CHUNK_COUNTER\n",
    "    all_chunks_processed = all_chunks.copy()\n",
    "    splitter = SentenceSplitter(\n",
    "                chunk_size=512,\n",
    "                chunk_overlap=100\n",
    "            )\n",
    "    for chunk in all_chunks_processed:\n",
    "        chunk['ID'] = CHUNK_COUNTER\n",
    "        CHUNK_COUNTER+=1\n",
    "        if chunk[\"Metadata\"]:\n",
    "            if chunk[\"Metadata\"]['is_table']:\n",
    "                chunk[\"Chunks\"]['child_chunks'] = {}\n",
    "                continue\n",
    "            text = chunk[\"Chunks\"][\"parent_chunk\"]\n",
    "            chunk[\"Chunks\"]['child_chunks'] = {}\n",
    "            documents = [Document(text = text)]\n",
    "            nodes = splitter.get_nodes_from_documents(documents)\n",
    "            for i,node in enumerate(nodes):\n",
    "                if text == node.text:\n",
    "                    break\n",
    "                if i==0:\n",
    "                    if len(text.split()) - len(node.text.split()) > 100:\n",
    "                        chunk[\"Chunks\"]['child_chunks'][f\"{i}\"] = node.text\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    if len(node.text.split()) > 50:\n",
    "                        chunk[\"Chunks\"]['child_chunks'][f\"{i}\"]=node.text\n",
    "                    else:\n",
    "                        chunk[\"Chunks\"]['child_chunks'][f\"{i-1}\"]+=node.text\n",
    "    return all_chunks_processed\n",
    "\n",
    "\n",
    "#We will is_table as true if 30% of all_items matches one of the tables\n",
    "def get_table_of_contents(chunk_obj):\n",
    "    def is_table(table,all_items):\n",
    "        q_items = all_items[:math.ceil(len(all_items)*0.3)]\n",
    "        table = table.get_text().lower()\n",
    "        for item in q_items:\n",
    "            if item.lower() not in table:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    all_items = chunk_obj.list_items()\n",
    "    table_of_contents = pd.DataFrame()\n",
    "    MAX_ITEMS = -1\n",
    "    ITEM_COLUMN_INDEX = -1\n",
    "    for table in chunk_obj.tables():\n",
    "        if is_table(table, all_items):\n",
    "            table_of_contents = table.to_dataframe()\n",
    "            #Naming the 'Items' columns and 'Headings' columns\n",
    "            for i in range(table_of_contents.shape[1]):\n",
    "                NUM_ITEMS = sum('item' in item.lower() for item in table_of_contents.iloc[:,i])\n",
    "                if NUM_ITEMS > MAX_ITEMS:\n",
    "                    ITEM_COLUMN_INDEX = i\n",
    "                    MAX_ITEMS = NUM_ITEMS\n",
    "            \n",
    "            #naming the columns\n",
    "            columns = [''] * table_of_contents.shape[1]\n",
    "            columns[ITEM_COLUMN_INDEX] = 'Index'\n",
    "            columns[ITEM_COLUMN_INDEX + 1] = 'Headings'\n",
    "\n",
    "            #setting the columns to datarame's column att\n",
    "            table_of_contents.columns = columns\n",
    "            table_of_contents = table_of_contents.set_index('Index')\n",
    "\n",
    "            return table_of_contents\n",
    "        \n",
    "def get_heading_dict(all_items, table_of_contents):\n",
    "    #Dictionary of items\n",
    "    dict = {}\n",
    "\n",
    "    #if table of contents is none, then we will return items themselves in keys and value both\n",
    "    if table_of_contents.empty :\n",
    "        for item in all_items:\n",
    "            dict[item] = item\n",
    "        return dict\n",
    "\n",
    "    for item in all_items:\n",
    "        for t_item in table_of_contents.index:\n",
    "            if bool(re.fullmatch(item.lower() + '\\W*',t_item.lower())):\n",
    "                dict[item] = item + ': ' + table_of_contents.loc[t_item,'Headings']\n",
    "                break\n",
    "    return dict\n",
    "    \n",
    "\n",
    "def chunk_document(chunk_obj, filing_date, company, item_heading_dictionary, form_type = '10-K'):\n",
    "    regex1 = re.compile(\n",
    "                    r\"\"\"\n",
    "                        \\n*                     # optional newlines at start\n",
    "                        .*?                     # company name\n",
    "                        \\s\\|\\s                  # ' | '\n",
    "                        .*?                     # Any text like 'Q2'\n",
    "                        \\d{4}                   # 4-digit year\n",
    "                        \\sForm\\s10-K            # ' Form 10-K'\n",
    "                        \\s\\|\\s                  # ' | '\n",
    "                        \\d+                     # page number\n",
    "                        \\n*                     # optional newlines at end\n",
    "                        $\"\"\",\n",
    "                    re.VERBOSE,\n",
    "                    )\n",
    "    # Regex 2: Matches only digits\n",
    "    # ^\\d+$\n",
    "    regex2 = re.compile(r\"\\n*\\d+\\n*$\")\n",
    "\n",
    "    metadata = {'item_heading':None, 'filing_date':filing_date, 'company':company, 'form':form_type, 'is_table':False}\n",
    "    #Get all the items present in the table\n",
    "    items = chunk_obj.list_items()\n",
    "\n",
    "    all_chunks = []\n",
    "    processed_chunk = []\n",
    "    count_of_words = 0\n",
    "    for item in items:\n",
    "        metadata['item_heading'] = item_heading_dictionary.get(item, item)\n",
    "        chunks_for_item = chunk_obj.chunks_for_item(item)\n",
    "        Headings = {}\n",
    "        prev_content = False\n",
    "        prev_heading = False\n",
    "        metadata_chunk = metadata.copy()\n",
    "        count_of_words = 0\n",
    "        for chunk in chunks_for_item:\n",
    "            for element in chunk:\n",
    "                #We want only text or tables\n",
    "                if isinstance(element, (TextBlock)):\n",
    "                    if element.is_header:\n",
    "                        if regex1.fullmatch(element.get_text()) or regex2.fullmatch(element.get_text()):\n",
    "                            Headings = {}\n",
    "                            continue\n",
    "\n",
    "                        text = element.get_text().strip('\\n').strip(' ')\n",
    "                        if item.lower() in text.lower() or 'part' in text.lower():\n",
    "                            continue\n",
    "                        if prev_content and processed_chunk:\n",
    "                            chunk_text = ' '.join(processed_chunk)\n",
    "                            count_of_words = 0\n",
    "                            if len(chunk_text.split()) > 20:\n",
    "                                all_chunks.append({\n",
    "                                                    'Metadata': metadata_chunk.copy(),\n",
    "                                                    'Chunks': {\"parent_chunk\":chunk_text}\n",
    "                                                })\n",
    "                            processed_chunk = []\n",
    "                            Headings = {}\n",
    "                        \n",
    "                        if Headings:\n",
    "                            max_level = max(list(Headings.keys()))\n",
    "                            Headings[max_level+1] = text\n",
    "                        else:\n",
    "                            Headings[1] = text\n",
    "                        \n",
    "                        prev_content = False\n",
    "                        prev_heading = True\n",
    "                    \n",
    "                    else:\n",
    "                        if prev_heading:\n",
    "                            metadata_chunk = metadata.copy()\n",
    "                            if len(Headings) > 3:\n",
    "                                chunk_text = ''\n",
    "                                for key, value in Headings.items():\n",
    "                                    chunk_text+=value + '\\n'\n",
    "                                all_chunks.append({\n",
    "                                                    'Metadata': metadata_chunk,\n",
    "                                                    'Chunks': {\"parent_chunk\":chunk_text}\n",
    "                                                })\n",
    "                            else:\n",
    "                                metadata_chunk['sub_headings'] = {}\n",
    "                                for key, value in Headings.items():\n",
    "                                    metadata_chunk['sub_headings']['level_'+str(key)+'_heading'] = value\n",
    "\n",
    "                        text = element.get_text().strip('\\n').strip(' ')\n",
    "                        processed_chunk.append(text)\n",
    "                        count_of_words += len(text.split())\n",
    "                        prev_content = True\n",
    "                        prev_heading = False\n",
    "\n",
    "                elif isinstance(element, TableBlock):\n",
    "                    metadata_chunk = metadata.copy()\n",
    "                    if prev_heading:\n",
    "                        if len(Headings) > 3:\n",
    "                            chunk_text = ''\n",
    "                            for key, value in Headings.items():\n",
    "                                chunk_text+=value + '\\n'\n",
    "                            all_chunks.append({\n",
    "                                                'Metadata': metadata_chunk,\n",
    "                                                'Chunks': {\"parent_chunk\":chunk_text}\n",
    "                                            })\n",
    "                        else:\n",
    "                            metadata_chunk['sub_headings'] = {}\n",
    "                            for key, value in Headings.items():\n",
    "                                metadata_chunk['sub_headings']['level_'+str(key)+'_heading'] = value\n",
    "                    \n",
    "                    chunk_text = ''\n",
    "                    if prev_content:\n",
    "                        text = '\\n'.join(processed_chunk)\n",
    "                        if len(text.split()) < 20 and len(text.split()) > 5:\n",
    "                            chunk_text = text\n",
    "\n",
    "                    chunk_text += element.get_text()\n",
    "                    metadata_chunk['is_table'] = True\n",
    "                    all_chunks.append({\n",
    "                                        'Metadata': metadata_chunk,\n",
    "                                        'Chunks': {\"parent_chunk\":chunk_text}\n",
    "                                    })\n",
    "                    prev_content = True\n",
    "                    prev_heading = False\n",
    "\n",
    "        if processed_chunk:\n",
    "            count_of_words = 0\n",
    "            chunk_text = '\\n'.join(processed_chunk)\n",
    "            if len(chunk_text.split()) > 20:\n",
    "                all_chunks.append({\n",
    "                                    'Metadata': metadata_chunk,\n",
    "                                    'Chunks': {\"parent_chunk\":chunk_text}\n",
    "                                })\n",
    "            processed_chunk = []\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "def get_chunks(filing_date: str,ticker:str, company: str, cik: int, accession_no:str) -> list:\n",
    "    filing = Filing(\n",
    "            form='10-K',\n",
    "            filing_date= filing_date,\n",
    "            company= ticker,\n",
    "            cik=cik,\n",
    "            accession_no= accession_no\n",
    "        )\n",
    "\n",
    "    obj = filing.obj()\n",
    "    chunk_obj = obj.chunked_document\n",
    "\n",
    "    table_of_contents = get_table_of_contents(chunk_obj)\n",
    "    item_heading_dictionary = get_heading_dict(sorted(chunk_obj.list_items()), table_of_contents)\n",
    "    all_chunks = chunk_document(chunk_obj, filing_date, company, item_heading_dictionary, form_type = '10-K')\n",
    "    \n",
    "    print(f\"Processed {ticker}, {filing_date}\")\n",
    "    return all_chunks\n",
    "\n",
    "def main():\n",
    "    global TOTAL_CHUNKS_TENK\n",
    "    for (cik, ticker),company in zip(TICKERS.items(),COMPANY_NAMES):\n",
    "        meta_df_path = os.path.join(BASE_PATH_TENK, ticker, '10-K.csv')\n",
    "        meta_df = pd.read_csv(meta_df_path)\n",
    "        meta_df = meta_df[pd.to_datetime(meta_df['filing_date']).dt.year > 2023]\n",
    "\n",
    "        for _, row in meta_df.iterrows():\n",
    "            filing_date = row['filing_date']\n",
    "            accession_no = row['accession_number']\n",
    "            all_chunks = get_chunks(filing_date, ticker, company, cik, accession_no)\n",
    "            TOTAL_CHUNKS_TENK+=len(all_chunks)\n",
    "            all_chunks_processed = post_processing_chunks(all_chunks)\n",
    "            \n",
    "            chunk_det = {\n",
    "                'filing_date' : filing_date,\n",
    "                'ticker' : ticker,\n",
    "                'file_chunks' : all_chunks_processed\n",
    "            }\n",
    "            \n",
    "            TENK_CHUNKS.append(chunk_det)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65146c27",
   "metadata": {},
   "source": [
    "### SAVE 10K Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378eca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Use 'w' mode for writing text\n",
    "with open(\"./tenk_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(TENK_CHUNKS, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf07ca3e",
   "metadata": {},
   "source": [
    "## 10-Q CHUNKING\n",
    "\n",
    "### Algorithm - \n",
    "1. Iterate over Parts one by one.\n",
    "2. Check in headings and tables for 'Item' headings and match with the Parts' items.\n",
    "3. Initialize a Dynamic meta data dictionary for every chunk with a 'part_header' : '{part_no, headings_text} and 'parent_header' : '{item_no, heading_text}', 'form_type':'10_K', 'Company':{company name}, 'filing_date':'{date}'\n",
    "3. If any element is a header, mention '--- heading' in front of that and join it back with the parent chunk.\n",
    "4. Every chunk should contain maximum 250 words.\n",
    "5. Every table should be a separate chunk.\n",
    "6. Mention table in the METADATA of table text and keep it a standalone text, no matter what the size is.\n",
    "\n",
    "### Cleaning Chunks.\n",
    "1. Remove the footers using this pattern '.* |.* |[0-9]*'\n",
    "2. Remove the text if they only contain a single number '[0-9]+', it will remove page numbers.\n",
    "3. Remove any text which contains '[a-z]* inc.' and has less than 3 words, lowercase the text for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae77f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AAPL, 2025-08-01\n",
      "Processed AAPL, 2025-05-02\n",
      "Processed AAPL, 2025-01-31\n",
      "Processed AAPL, 2024-08-02\n",
      "Processed AAPL, 2024-05-03\n",
      "Processed AAPL, 2024-02-02\n",
      "Processed MSFT, 2025-10-29\n",
      "Processed MSFT, 2025-04-30\n",
      "Processed MSFT, 2025-01-29\n",
      "Processed MSFT, 2024-10-30\n",
      "Processed MSFT, 2024-04-25\n",
      "Processed MSFT, 2024-01-30\n",
      "Processed GOOGL, 2025-10-30\n",
      "Processed GOOGL, 2025-07-24\n",
      "Processed GOOGL, 2025-04-25\n",
      "Processed GOOGL, 2024-10-30\n",
      "Processed GOOGL, 2024-07-24\n",
      "Processed GOOGL, 2024-04-26\n",
      "Processed AMZN, 2025-10-31\n",
      "Processed AMZN, 2025-08-01\n",
      "Processed AMZN, 2025-05-02\n",
      "Processed AMZN, 2024-11-01\n",
      "Processed AMZN, 2024-08-02\n",
      "Processed AMZN, 2024-05-01\n",
      "Processed META, 2025-10-30\n",
      "Processed META, 2025-07-31\n",
      "Processed META, 2025-05-01\n",
      "Processed META, 2024-10-31\n",
      "Processed META, 2024-08-01\n",
      "Processed META, 2024-04-25\n",
      "Processed NVDA, 2025-11-19\n",
      "Processed NVDA, 2025-08-27\n",
      "Processed NVDA, 2025-05-28\n",
      "Processed NVDA, 2024-11-20\n",
      "Processed NVDA, 2024-08-28\n",
      "Processed NVDA, 2024-05-29\n",
      "Processed TSLA, 2025-10-23\n",
      "Processed TSLA, 2025-07-24\n",
      "Processed TSLA, 2025-04-23\n",
      "Processed TSLA, 2024-10-24\n",
      "Processed TSLA, 2024-07-24\n",
      "Processed TSLA, 2024-04-24\n",
      "Processed ORCL, 2025-09-10\n",
      "Processed ORCL, 2025-03-11\n",
      "Processed ORCL, 2024-12-10\n",
      "Processed ORCL, 2024-09-10\n",
      "Processed ORCL, 2024-03-12\n",
      "Processed CRM, 2025-12-04\n",
      "Processed CRM, 2025-09-04\n",
      "Processed CRM, 2025-05-29\n",
      "Processed CRM, 2024-12-04\n",
      "Processed CRM, 2024-08-29\n",
      "Processed CRM, 2024-05-30\n",
      "Processed NFLX, 10/22/2025\n",
      "Processed NFLX, 7/18/2025\n",
      "Processed NFLX, 4/18/2025\n",
      "Processed NFLX, 10/18/2024\n",
      "Processed NFLX, 7/19/2024\n",
      "Processed NFLX, 4/22/2024\n",
      "Processed ADBE, 2025-09-24\n",
      "Processed ADBE, 2025-06-25\n",
      "Processed ADBE, 2025-03-26\n",
      "Processed ADBE, 2024-09-25\n",
      "Processed ADBE, 2024-06-26\n",
      "Processed ADBE, 2024-03-27\n"
     ]
    }
   ],
   "source": [
    "metadata_csv = pd.DataFrame()\n",
    "\n",
    "\n",
    "COMPANY_NAMES = [\n",
    "        \"Apple Inc.\",\n",
    "        \"MICROSOFT CORP\",\n",
    "        \"Alphabet Inc.\",\n",
    "        \"AMAZON COM INC.\",\n",
    "        \"Meta Platforms, Inc.\",\n",
    "        \"NVIDIA CORP\",\n",
    "        \"Tesla, Inc.\",\n",
    "        \"ORACLE CORP\",\n",
    "        \"Salesforce, Inc.\",\n",
    "        \"NETFLIX INC\",\n",
    "        \"ADOBE INC.\"\n",
    "    ]\n",
    "\n",
    "TICKERS = {\n",
    "        320193 : \"AAPL\",\n",
    "        789019: \"MSFT\",\n",
    "        1652044: \"GOOGL\",\n",
    "        1018724: \"AMZN\",\n",
    "        1326801 : \"META\",\n",
    "        1045810: \"NVDA\",\n",
    "        1318605 : \"TSLA\",\n",
    "        1341439 : \"ORCL\",\n",
    "        1108524 : \"CRM\",\n",
    "        1065280 : \"NFLX\",\n",
    "        796343 : \"ADBE\"\n",
    "    }\n",
    "\n",
    "\n",
    "#Post processing the exisiting chunks\n",
    "def post_processing_chunks(all_chunks):\n",
    "    global CHUNK_COUNTER\n",
    "    all_chunks_processed = all_chunks.copy()\n",
    "    splitter = SentenceSplitter(\n",
    "                chunk_size=512,\n",
    "                chunk_overlap=100\n",
    "            )\n",
    "    for chunk in all_chunks_processed:\n",
    "        chunk['ID'] = CHUNK_COUNTER\n",
    "        CHUNK_COUNTER+=1\n",
    "        if chunk[\"Metadata\"]:\n",
    "            if chunk[\"Metadata\"]['is_table']:\n",
    "                chunk[\"Chunks\"]['child_chunks'] = {}\n",
    "                continue\n",
    "            text = chunk[\"Chunks\"][\"parent_chunk\"]\n",
    "            chunk[\"Chunks\"]['child_chunks'] = {}\n",
    "            documents = [Document(text = text)]\n",
    "            nodes = splitter.get_nodes_from_documents(documents)\n",
    "            for i,node in enumerate(nodes):\n",
    "                if text == node.text:\n",
    "                    break\n",
    "                if i==0:\n",
    "                    if len(text.split()) - len(node.text.split()) > 100:\n",
    "                        chunk[\"Chunks\"]['child_chunks'][f\"{i}\"] = node.text\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    if len(node.text.split()) > 50:\n",
    "                        chunk[\"Chunks\"]['child_chunks'][f\"{i}\"]=node.text\n",
    "                    else:\n",
    "                        chunk[\"Chunks\"]['child_chunks'][f\"{i-1}\"]+=node.text\n",
    "    return all_chunks_processed\n",
    "\n",
    "#We will is_table as true if 30% of all_items matches one of the tables\n",
    "def get_table_of_contents(chunk_obj):\n",
    "    def is_table(table,all_items):\n",
    "        q_items = all_items[:math.ceil(len(all_items)*0.3)]\n",
    "        table = table.get_text().lower()\n",
    "        for item in q_items:\n",
    "            if item.lower() not in table:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    all_items = chunk_obj.list_items()\n",
    "    table_of_contents = pd.DataFrame()\n",
    "    MAX_ITEMS = -1\n",
    "    ITEM_COLUMN_INDEX = -1\n",
    "    for table in chunk_obj.tables():\n",
    "        if is_table(table, all_items):\n",
    "            table_of_contents = table.to_dataframe()\n",
    "            \n",
    "            #Naming the 'Items' columns and 'Headings' columns\n",
    "            for i in range(table_of_contents.shape[1]):\n",
    "                NUM_ITEMS = sum('item' in item.lower() for item in table_of_contents.iloc[:,i])\n",
    "                if NUM_ITEMS > MAX_ITEMS:\n",
    "                    ITEM_COLUMN_INDEX = i\n",
    "                    MAX_ITEMS = NUM_ITEMS\n",
    "            \n",
    "            #Naming the columns\n",
    "            columns = [''] * table_of_contents.shape[1]\n",
    "            columns[ITEM_COLUMN_INDEX] = 'Index'\n",
    "            columns[ITEM_COLUMN_INDEX + 1] = 'Headings'\n",
    "\n",
    "            #Setting the columns to datarame's column attribute\n",
    "            table_of_contents.columns = columns\n",
    "            \n",
    "            return table_of_contents\n",
    "\n",
    "def get_correct_item(item, all_items):\n",
    "    for actual_item in all_items:\n",
    "        if re.fullmatch(item+'[^a-zA-Z0-9]*',actual_item):\n",
    "            return actual_item\n",
    "    \n",
    "    item = item.strip('. ').split()\n",
    "    item = item[0].capitalize() + ' ' + item[1]\n",
    "    return item\n",
    "\n",
    "\n",
    "\n",
    "def get_heading_dict(all_items, table_of_contents):\n",
    "    #Dictionary of items\n",
    "    headings = {}\n",
    "    part_1 = False\n",
    "    part_2 = False\n",
    "\n",
    "    #if table of contents is none, then we will return items themselves in keys and value both\n",
    "    if table_of_contents.empty :\n",
    "        headings['Part I'] = {'Heading':'Financial Information','Items':[]}\n",
    "        headings['Part II'] = {'Heading':'Other Information','Items':[]}\n",
    "        return headings\n",
    "\n",
    "    for _, row in table_of_contents.iterrows() :\n",
    "        if re.fullmatch(r'part i[^a-zA-Z0-9]*', row['Index'].lower()) or 'financial information' in row['Index'].lower() and not part_1:\n",
    "            headings['Part I'] = {'Heading':'Part I: Financial Information','Items':{}}\n",
    "            part_1, part_2 = True, False\n",
    "            \n",
    "        elif re.fullmatch(r'part i[^a-zA-Z0-9]*', row['Headings'].lower()) or 'financial information' in row['Headings'].lower() and not part_1:\n",
    "            headings['Part I'] = {'Heading':'Part I: Financial Information','Items':{}}\n",
    "            part_1, part_2 = True, False\n",
    "\n",
    "        elif re.fullmatch(r'part ii[^a-zA-Z0-9]*', row['Index'].lower()) or 'other information' in row['Index'].lower() and not part_2:\n",
    "            headings['Part II'] = {'Heading':'Part II: Other Information','Items':{}}\n",
    "            part_1, part_2 = False, True\n",
    "\n",
    "        elif re.fullmatch(r'part ii[^a-zA-Z0-9]*', row['Headings'].lower()) or 'other information' in row['Headings'].lower() and not part_2:\n",
    "            headings['Part II'] = {'Heading':'Part II: Other Information','Items':{}}\n",
    "            part_1, part_2 = False, True\n",
    "\n",
    "        if part_1:\n",
    "            item = row['Index'].strip('.')\n",
    "            heading = row['Headings']\n",
    "            if 'item' in item.lower():\n",
    "                actual_item = get_correct_item(item,all_items)\n",
    "                headings['Part I']['Items'][actual_item] = actual_item + ': '+ heading.strip(':. \\n')\n",
    "\n",
    "        elif part_2:\n",
    "            item = row['Index'].strip('.')\n",
    "            heading = row['Headings']\n",
    "            if 'item' in item.lower():\n",
    "                actual_item = get_correct_item(item,all_items)\n",
    "                headings['Part II']['Items'][actual_item] = actual_item + ': '+ heading.strip(':. \\n')\n",
    "\n",
    "    return headings\n",
    "    \n",
    "\n",
    "def chunk_document(chunk_obj, filing_date, company, heading_dictionary, form_type = '10-Q'):\n",
    "\n",
    "    regex1 = re.compile(\n",
    "                        r\"\"\"\n",
    "                            \\n*                     # optional newlines at start\n",
    "                            .*?                     # company name\n",
    "                            \\s\\|\\s                  # ' | '\n",
    "                            .*?                     # Any text like 'Q2'\n",
    "                            \\d{4}                   # 4-digit year\n",
    "                            \\sForm\\s10-Q            # ' Form 10-K'\n",
    "                            \\s\\|\\s                  # ' | '\n",
    "                            \\d+                     # page number\n",
    "                            \\n*                     # optional newlines at end\n",
    "                            $\"\"\",\n",
    "                        re.VERBOSE,\n",
    "                        )\n",
    "\n",
    "    # Regex 2: Matches only digits\n",
    "    # ^\\d+$\n",
    "    regex2 = re.compile(r\"^\\n*\\d+\\n*$\")\n",
    "\n",
    "    metadata = {'part_heading':None,'item_heading':None, 'filing_date':filing_date, 'company':company, 'form':form_type, 'is_table':False}\n",
    "    #Get all the items present in the table\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    processed_chunk = []\n",
    "    prev_heading = {'T/F':False,'Heading': None}\n",
    "    count_of_words = 0\n",
    "    \n",
    "    for part in ['Part I','Part II']:\n",
    "        metadata['part_heading'] = heading_dictionary[part]['Heading']\n",
    "        part_items = list(heading_dictionary[part]['Items'].keys())\n",
    "        chunk_for_part = chunk_obj.chunks_for_part(part)\n",
    "        \n",
    "        Headings = {}\n",
    "        prev_content = False\n",
    "        prev_heading = False\n",
    "        metadata_chunk = metadata.copy()\n",
    "        count_of_words = 0\n",
    "        \n",
    "        for chunk in chunk_for_part:\n",
    "            for element in chunk:\n",
    "                #We want only text or tables\n",
    "                if isinstance(element, (TextBlock)):\n",
    "                    if element.is_header:\n",
    "                        if regex1.fullmatch(element.get_text()) or regex2.fullmatch(element.get_text()):\n",
    "                            Headings = {}\n",
    "                            continue\n",
    "\n",
    "                        #Extracting Items                        \n",
    "                        for item in part_items:\n",
    "                            item_heading = re.sub(\"item\\s*\\d*\\W*:\\s*\",\"\",heading_dictionary[part]['Items'][item].lower())\n",
    "                            if item_heading in element.get_text().lower():\n",
    "                                if processed_chunk:\n",
    "                                    count_of_words = 0\n",
    "                                    chunk_text = '\\n'.join(processed_chunk)\n",
    "                                    if len(chunk_text.split()) > 20:\n",
    "                                        all_chunks.append({\n",
    "                                                            'Metadata': metadata_chunk,\n",
    "                                                            'Chunks': {\"parent_chunk\":chunk_text}\n",
    "                                                        })\n",
    "                                        processed_chunk = []\n",
    "                                metadata['item_heading'] = heading_dictionary[part]['Items'][item]\n",
    "                        \n",
    "                        \n",
    "                        text = element.get_text().strip('\\n').strip(' ')\n",
    "\n",
    "                        if 'item' in text.lower() or 'part' in text.lower():\n",
    "                            continue\n",
    "\n",
    "                        if prev_content and processed_chunk:\n",
    "                            chunk_text = ' '.join(processed_chunk)\n",
    "                            count_of_words = 0\n",
    "                            if len(chunk_text.split()) > 20:\n",
    "                                all_chunks.append({\n",
    "                                                    'Metadata': metadata_chunk.copy(),\n",
    "                                                    'Chunks': {\"parent_chunk\":chunk_text}\n",
    "                                                })\n",
    "                            processed_chunk = []\n",
    "                            Headings = {}\n",
    "                        \n",
    "                        if Headings:\n",
    "                            max_level = max(list(Headings.keys()))\n",
    "                            Headings[max_level+1] = text\n",
    "                        else:\n",
    "                            Headings[1] = text\n",
    "                        \n",
    "                        prev_content = False\n",
    "                        prev_heading = True\n",
    "                    \n",
    "                    else:\n",
    "                        if prev_heading:\n",
    "                            metadata_chunk = metadata.copy()\n",
    "                            if len(Headings) > 3:\n",
    "                                chunk_text = ''\n",
    "                                for key, value in Headings.items():\n",
    "                                    chunk_text+=value + '\\n'\n",
    "                                all_chunks.append({\n",
    "                                                    'Metadata': metadata_chunk,\n",
    "                                                    'Chunks': {\"parent_chunk\":chunk_text}\n",
    "                                                })\n",
    "                            else:\n",
    "                                metadata_chunk['sub_headings'] = {}\n",
    "                                for key, value in Headings.items():\n",
    "                                    metadata_chunk['sub_headings']['level_'+str(key)+'_heading'] = value\n",
    "\n",
    "                        text = element.get_text().strip('\\n').strip(' ')\n",
    "                        processed_chunk.append(text)\n",
    "                        count_of_words += len(text.split())\n",
    "                        prev_content = True\n",
    "                        prev_heading = False\n",
    "\n",
    "                elif isinstance(element, TableBlock):\n",
    "                    #Update Metadata parent_heading key\n",
    "                    for item in part_items:\n",
    "                        item_heading = re.sub(\"item\\s*\\d*\\W*:\\s*\",\"\",heading_dictionary[part]['Items'][item].lower())\n",
    "                        if item_heading in element.get_text().lower():\n",
    "                            if processed_chunk:\n",
    "                                chunk_text = '\\n'.join(processed_chunk)\n",
    "                                if len(chunk_text.split()) > 20:\n",
    "                                    all_chunks.append({\n",
    "                                                        'Metadata': metadata_chunk,\n",
    "                                                        'Chunks': {\"parent_chunk\":chunk_text}\n",
    "                                                    })\n",
    "                                processed_chunk = []\n",
    "                                count_of_words = 0\n",
    "                            metadata['item_heading'] = heading_dictionary[part]['Items'][item]\n",
    "\n",
    "                    if element.get_text().lower().count('item') < 6:\n",
    "                        metadata_chunk = metadata.copy()\n",
    "                        if prev_heading or prev_content:\n",
    "                            if len(Headings) > 3:\n",
    "                                chunk_text = ''\n",
    "                                for key, value in Headings.items():\n",
    "                                    chunk_text+=value + '\\n'\n",
    "                                all_chunks.append({\n",
    "                                                    'Metadata': metadata_chunk,\n",
    "                                                    'Chunks': {\"parent_chunk\":chunk_text}\n",
    "                                                })\n",
    "                            else:\n",
    "                                metadata_chunk['sub_headings'] = {}\n",
    "                                for key, value in Headings.items():\n",
    "                                    metadata_chunk['sub_headings']['level_'+str(key)+'_heading'] = value\n",
    "\n",
    "                        chunk_text = ''\n",
    "                        if prev_content:\n",
    "                            text = '\\n'.join(processed_chunk)\n",
    "                            if len(text.split()) < 20 and len(text.split()) > 5:\n",
    "                                chunk_text = text\n",
    "\n",
    "                        chunk_text += element.get_text()\n",
    "                        \n",
    "                        metadata_chunk['is_table'] = True\n",
    "                        all_chunks.append({\n",
    "                                            'Metadata': metadata_chunk,\n",
    "                                            'Chunks': {\"parent_chunk\":chunk_text}\n",
    "                                        })\n",
    "                        prev_content = True\n",
    "                        prev_heading = False\n",
    "\n",
    "        if processed_chunk:\n",
    "            count_of_words = 0\n",
    "            chunk_text = '\\n'.join(processed_chunk)\n",
    "            if len(chunk_text.split()) > 20:\n",
    "                all_chunks.append({\n",
    "                                    'Metadata': metadata_chunk,\n",
    "                                    'Chunks': {\"parent_chunk\":chunk_text}\n",
    "                                })\n",
    "            processed_chunk = []\n",
    "            \n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "def get_chunks(filing_date: str,ticker:str, company: str, cik: int, accession_no:str) -> list:\n",
    "    # print(filing_date, ticker, cik, accession_no)\n",
    "    filing = Filing(\n",
    "            form='10-Q',\n",
    "            filing_date= filing_date,\n",
    "            company= ticker,\n",
    "            cik=cik,\n",
    "            accession_no= accession_no\n",
    "        )\n",
    "\n",
    "    obj = filing.obj()\n",
    "    chunk_obj = obj.chunked_document\n",
    "\n",
    "    table_of_contents = get_table_of_contents(chunk_obj)\n",
    "    heading_dictionary = get_heading_dict(sorted(chunk_obj.list_items()),table_of_contents)\n",
    "    all_chunks = chunk_document(chunk_obj, filing_date, company, heading_dictionary, form_type = '10-Q')\n",
    "    \n",
    "    print(f\"Processed {ticker}, {filing_date}\")\n",
    "    return all_chunks\n",
    "\n",
    "def main():\n",
    "    global TOTAL_CHUNKS_TENQ\n",
    "    for (cik, ticker),company in zip(TICKERS.items(),COMPANY_NAMES):\n",
    "        meta_df_path = os.path.join(BASE_PATH_TENQ, ticker, '10-Q.csv')\n",
    "        meta_df = pd.read_csv(meta_df_path)\n",
    "        meta_df = meta_df[pd.to_datetime(meta_df['filing_date']).dt.year > 2023]\n",
    "\n",
    "        for _, row in meta_df.iterrows():\n",
    "            filing_date = row['filing_date']\n",
    "            accession_no = row['accession_number']\n",
    "            all_chunks = get_chunks(filing_date, ticker, company, cik, accession_no)\n",
    "            TOTAL_CHUNKS_TENQ+=len(all_chunks)\n",
    "            all_chunks_processed = post_processing_chunks(all_chunks)\n",
    "            \n",
    "            chunk_det = {\n",
    "                'filing_date' : filing_date,\n",
    "                'ticker' : ticker,\n",
    "                'file_chunks' : all_chunks_processed\n",
    "            }\n",
    "            \n",
    "            TENQ_CHUNKS.append(chunk_det)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473bcf1f",
   "metadata": {},
   "source": [
    "### SAVE 10-Q CHUNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Use 'w' mode for writing text\n",
    "with open(\"./tenq_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(TENQ_CHUNKS, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbe6a681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13745"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_CHUNKS_TENK + TOTAL_CHUNKS_TENQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9b420",
   "metadata": {},
   "source": [
    "## Save all Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f3783",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenk_tenq_chunks = TENK_CHUNKS + TENQ_CHUNKS\n",
    "\n",
    "with open(\"./tenk_tenq_chuks.json\",'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(tenk_tenq_chunks, f, indent = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantigence-oNV_nXK--py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
