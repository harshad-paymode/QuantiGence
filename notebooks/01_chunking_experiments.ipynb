{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e06c7ae",
   "metadata": {},
   "source": [
    "# CHUNKING STRATEGY\n",
    "RULES OF CHUNKING - \n",
    "1. Process every document separately.\n",
    "2. Remove all elements before 'PART I' Business Strategy, 'Table of Contents', and 'Page Numbers'.\n",
    "3. Chunk all lowest-level headings. Implement a 20% overlap.\n",
    "4. Merge two chunks if one has less than 200 tokens.\n",
    "5. Add the full hierarchical path of all headings/subheadings and document metadata to the metadata tag.\n",
    "(e.g., document_id, fiscal_year, part, section_1, section_2, section_3)\n",
    "6. Send the chunks along with metadata information to the LLM.\n",
    "7. Architectural Layer: Implement a Parent-Child Retrieval mechanism where search is done on the fine-grained chunks, but chunks under second-level heading will be combined and sent to LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfe317a",
   "metadata": {},
   "source": [
    "## 10-K CHUNKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126c9405",
   "metadata": {},
   "source": [
    "### Algorithm - \n",
    "1. Iterate through every item one by one.\n",
    "2. Initialize a Dynamic meta data dictionary for every chunk with a 'parent_header' : '{item_no: text}', 'form_type':'10_K', 'Company':{company name}, 'filing_date':'{date}'\n",
    "3. If any element is a header, mention '--- heading' in front of that and join it back with the parent chunk.\n",
    "4. Every chunk should contain maximum 250 words.\n",
    "5. Mention table in the METADATA of table text and keep it a standalone text, no matter what the size is.\n",
    "\n",
    "### Cleaning Chunks -\n",
    "1. Remove the footers using this pattern '.* |.* |[0-9]*'\n",
    "2. Remove the text if they only contain a single number '[0-9]+', it will remove page numbers.\n",
    "3. Remove any text which contains '[a-z]* inc.' and has less than 3 words, lowercase the text for this.\n",
    "\n",
    "### Further Refinements (Semantic Chunking) - \n",
    "1. For every Heading.\n",
    "2. Check if the dictionary of headings is empty, if yes append level 1 heading to the dict.\n",
    "3. Else check for the max heading num, then append it as max + 1.\n",
    "4. Check if previous chunk was content and has atleast 10 words, then empty the heading dict and append a new heading in level '1' also append the chunk as a new one and empty curr_chunk.\n",
    "5. While appending content check if the chunk size is 200 words, append if yes. Else continue adding.\n",
    "6. If consecutively more than 3 headings occur, continue till the next content appears, append all headings as content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "315d24a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paymo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\quantigence-oNV_nXK--py3.11\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from edgar import set_identity, Filing, find, use_local_storage, set_local_storage_path\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from itertools import chain\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import math\n",
    "from edgar.files.html_documents import TextBlock, TableBlock\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Document\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f430e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_COUNTER = 1\n",
    "TENK_TENQ_CHUNKS = []\n",
    "TENK_CHUNKS = []\n",
    "TENQ_CHUNKS = []\n",
    "TOTAL_CHUNKS_TENK = 0\n",
    "TOTAL_CHUNKS_TENQ = 0\n",
    "#Reading all the metadata from 10-K\n",
    "BASE_PATH_TENK = os.getenv(\"BASE_PATH_TENK\")\n",
    "BASE_PATH_TENQ = os.getenv(\"BASE_PATH_TENQ\")\n",
    "\n",
    "SAVE_PATH_TENK = os.getenv(\"OUT_10K\")\n",
    "SAVE_PATH_TENQ = os.getenv(\"OUT_10Q\")\n",
    "SAVE_PATH_ALL = os.getenv(\"OUT_ALL\")\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "set_identity(os.getenv(\"SEC_ID\"))\n",
    "use_local_storage(os.getenv(\"SEC_CACHE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b12a65ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AAPL, 2025-10-31\n",
      "Processed AAPL, 2024-11-01\n",
      "Processed MSFT, 2025-07-30\n",
      "Processed MSFT, 2024-07-30\n",
      "Processed GOOGL, 2025-02-05\n",
      "Processed GOOGL, 2024-01-31\n",
      "Processed AMZN, 2025-02-07\n",
      "Processed AMZN, 2024-02-02\n",
      "Processed META, 2025-01-30\n",
      "Processed META, 2024-02-02\n",
      "Processed NVDA, 2025-02-26\n",
      "Processed NVDA, 2024-02-21\n",
      "Processed TSLA, 2025-04-30\n",
      "Processed TSLA, 2025-01-30\n",
      "Processed TSLA, 2024-01-29\n",
      "Processed ORCL, 2025-06-18\n",
      "Processed ORCL, 2024-06-20\n",
      "Processed CRM, 2025-03-05\n",
      "Processed CRM, 2024-03-06\n",
      "Processed NFLX, 2025-01-27\n",
      "Processed NFLX, 2024-01-26\n",
      "Processed ADBE, 2025-01-13\n",
      "Processed ADBE, 2024-01-17\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_FORMAT = \"%Y-%m-%d\"\n",
    "\n",
    "COMPANY_NAMES = [\n",
    "        \"Apple Inc.\",\n",
    "        \"MICROSOFT CORP\",\n",
    "        \"Alphabet Inc.\",\n",
    "        \"AMAZON COM INC.\",\n",
    "        \"Meta Platforms, Inc.\",\n",
    "        \"NVIDIA CORP\",\n",
    "        \"Tesla, Inc.\",\n",
    "        \"ORACLE CORP\",\n",
    "        \"Salesforce, Inc.\",\n",
    "        \"NETFLIX INC\",\n",
    "        \"ADOBE INC.\"\n",
    "    ]\n",
    "\n",
    "TICKERS = {\n",
    "        320193 : \"AAPL\",\n",
    "        789019: \"MSFT\",\n",
    "        1652044: \"GOOGL\",\n",
    "        1018724: \"AMZN\",\n",
    "        1326801 : \"META\",\n",
    "        1045810: \"NVDA\",\n",
    "        1318605 : \"TSLA\",\n",
    "        1341439 : \"ORCL\",\n",
    "        1108524 : \"CRM\",\n",
    "        1065280 : \"NFLX\",\n",
    "        796343 : \"ADBE\"\n",
    "    }\n",
    "\n",
    "\n",
    "#Clean Text\n",
    "def clean_abnormal_text(text):\n",
    "    # 1. Remove long repetitive sequences (3+ dashes, dots, or symbols)\n",
    "    # This cleans up the \"----\" and \"......\" patterns common in broken tables\n",
    "    text = re.sub(r'[-._*~=]{3,}', ' ', text)\n",
    "    \n",
    "    # 2. Remove \"DAU/MAU\" noise blocks (appears in facebook)\n",
    "    # Specifically targets \"DAU/MAU:\" followed by numbers and percentages\n",
    "    text = re.sub(r'DAU/MAU:[\\d% \\-]+', ' ', text)\n",
    "    \n",
    "    # 3. Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text) # Normalize all whitespace to single spaces\n",
    "    \n",
    "    # 4. Remove isolated clusters of special characters\n",
    "    # This removes things like \"- - - \" that linger after other steps\n",
    "    text = re.sub(r'\\s([^\\w\\s])\\s', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "\n",
    "#Post processing the exisiting chunks\n",
    "def post_processing_chunks(all_chunks):\n",
    "    global CHUNK_COUNTER\n",
    "    all_chunks_processed = all_chunks.copy()\n",
    "    splitter = SentenceSplitter(\n",
    "                chunk_size=300,\n",
    "                chunk_overlap=60\n",
    "            )\n",
    "    for chunk in all_chunks_processed:\n",
    "        chunk['ID'] = CHUNK_COUNTER\n",
    "        CHUNK_COUNTER+=1\n",
    "        if chunk[\"Metadata\"]:\n",
    "            text = chunk[\"Chunks\"][\"parent_chunk\"]\n",
    "            chunk[\"Chunks\"]['child_chunks'] = {}\n",
    "\n",
    "            if chunk[\"Metadata\"]['is_table']:\n",
    "                chunk[\"Chunks\"]['child_chunks'][f\"{chunk['ID']}_{0}\"] = text\n",
    "                continue\n",
    "            \n",
    "            text  = clean_abnormal_text(text)\n",
    "            documents = [Document(text = text)]\n",
    "            nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "            for i,node in enumerate(nodes):\n",
    "                child_chunk_dict = chunk[\"Chunks\"]['child_chunks']\n",
    "                if i==0:\n",
    "                    child_chunk_dict[f\"{chunk['ID']}_{i}\"]=node.text\n",
    "                elif i>0:\n",
    "                    if len(node.text.split()) > 50:\n",
    "                        child_chunk_dict[f\"{chunk['ID']}_{i}\"]=node.text\n",
    "                    elif node.text not in child_chunk_dict[list(child_chunk_dict)[-1]]:\n",
    "                        child_chunk_dict[list(child_chunk_dict)[-1]]+=node.text\n",
    "                        \n",
    "    return all_chunks_processed\n",
    "\n",
    "\n",
    "#We will is_table as true if 30% of all_items matches one of the tables\n",
    "def get_table_of_contents(chunk_obj):\n",
    "    def is_table(table,all_items):\n",
    "        q_items = all_items[:math.ceil(len(all_items)*0.3)]\n",
    "        table = table.get_text().lower()\n",
    "        for item in q_items:\n",
    "            if item.lower() not in table:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    all_items = chunk_obj.list_items()\n",
    "    table_of_contents = pd.DataFrame()\n",
    "    MAX_ITEMS = -1\n",
    "    ITEM_COLUMN_INDEX = -1\n",
    "    for table in chunk_obj.tables():\n",
    "        if is_table(table, all_items):\n",
    "            table_of_contents = table.to_dataframe()\n",
    "            #Naming the 'Items' columns and 'Headings' columns\n",
    "            for i in range(table_of_contents.shape[1]):\n",
    "                NUM_ITEMS = sum('item' in item.lower() for item in table_of_contents.iloc[:,i])\n",
    "                if NUM_ITEMS > MAX_ITEMS:\n",
    "                    ITEM_COLUMN_INDEX = i\n",
    "                    MAX_ITEMS = NUM_ITEMS\n",
    "            \n",
    "            #naming the columns\n",
    "            columns = [''] * table_of_contents.shape[1]\n",
    "            columns[ITEM_COLUMN_INDEX] = 'Index'\n",
    "            columns[ITEM_COLUMN_INDEX + 1] = 'Headings'\n",
    "\n",
    "            #setting the columns to datarame's column att\n",
    "            table_of_contents.columns = columns\n",
    "            table_of_contents = table_of_contents.set_index('Index')\n",
    "\n",
    "            return table_of_contents\n",
    "        \n",
    "def get_heading_dict(all_items, table_of_contents):\n",
    "    #Dictionary of items\n",
    "    dict = {}\n",
    "\n",
    "    #if table of contents is none, then we will return items themselves in keys and value both\n",
    "    if table_of_contents.empty :\n",
    "        for item in all_items:\n",
    "            dict[item] = item\n",
    "        return dict\n",
    "\n",
    "    for item in all_items:\n",
    "        for t_item in table_of_contents.index:\n",
    "            if bool(re.fullmatch(item.lower() + '\\W*',t_item.lower())):\n",
    "                dict[item] = item + ': ' + table_of_contents.loc[t_item,'Headings']\n",
    "                break\n",
    "    return dict\n",
    "\n",
    "#Create chunk function\n",
    "def emit_chunk(all_chunks, metadata, text):\n",
    "    if len(text.split()) > 20:\n",
    "        all_chunks.append({\n",
    "            \"Metadata\": deepcopy(metadata),\n",
    "            \"Chunks\": {\"parent_chunk\": text}\n",
    "        })\n",
    "    metadata.pop(\"sub_headings\", None)\n",
    "    metadata[\"is_table\"] = False\n",
    "    return all_chunks, metadata\n",
    "\n",
    "def chunk_document(chunk_obj, filing_date, company, item_heading_dictionary,accession_no, form_type = '10-K'):\n",
    "    regex1 = re.compile(\n",
    "                    r\"\"\"\n",
    "                        \\n*                     # optional newlines at start\n",
    "                        .*?                     # company name\n",
    "                        \\s\\|\\s                  # ' | '\n",
    "                        .*?                     # Any text like 'Q2'\n",
    "                        \\d{4}                   # 4-digit year\n",
    "                        \\sForm\\s10-K            # ' Form 10-K'\n",
    "                        \\s\\|\\s                  # ' | '\n",
    "                        \\d+                     # page number\n",
    "                        \\n*                     # optional newlines at end\n",
    "                        $\"\"\",\n",
    "                    re.VERBOSE,\n",
    "                    )\n",
    "    # Regex 2: Matches only digits\n",
    "    # ^\\d+$\n",
    "    regex2 = re.compile(r\"\\n*\\d+\\n*$\")\n",
    "\n",
    "    metadata = {'item_heading':None, 'filing_date':filing_date, 'company':company, 'form':form_type, 'is_table':False,\"accession_no\":accession_no}\n",
    "    #Get all the items present in the table\n",
    "    items = chunk_obj.list_items()\n",
    "\n",
    "    all_chunks = []\n",
    "    processed_chunk = []\n",
    "    count_of_words = 0\n",
    "    for item in items:\n",
    "        metadata['item_heading'] = item_heading_dictionary.get(item, item)\n",
    "        chunks_for_item = chunk_obj.chunks_for_item(item)\n",
    "        Headings = {}\n",
    "        prev_content = False\n",
    "        prev_heading = False\n",
    "        metadata = metadata.copy()\n",
    "        count_of_words = 0\n",
    "        for chunk in chunks_for_item:\n",
    "            for element in chunk:\n",
    "                #We want only text or tables\n",
    "                if isinstance(element, (TextBlock)):\n",
    "                    if element.is_header:\n",
    "                        if regex1.fullmatch(element.get_text()) or regex2.fullmatch(element.get_text()):\n",
    "                            Headings = {}\n",
    "                            continue\n",
    "\n",
    "                        text = element.get_text().strip('\\n').strip(' ')\n",
    "                        if item.lower() in text.lower() or 'part' in text.lower():\n",
    "                            continue\n",
    "                        if prev_content and processed_chunk:\n",
    "                            chunk_text = ' '.join(processed_chunk)\n",
    "                            count_of_words = 0\n",
    "                            if len(chunk_text.split()) > 20:\n",
    "                                all_chunks,metadata = emit_chunk(all_chunks, metadata, chunk_text)\n",
    "                            processed_chunk = []\n",
    "                            Headings = {}\n",
    "                        \n",
    "                        if Headings:\n",
    "                            max_level = max(list(Headings.keys()))\n",
    "                            Headings[max_level+1] = text\n",
    "                        else:\n",
    "                            Headings[1] = text\n",
    "                        \n",
    "                        prev_content = False\n",
    "                        prev_heading = True\n",
    "                    \n",
    "                    else:\n",
    "                        if prev_heading:\n",
    "                            if len(Headings) > 3:\n",
    "                                chunk_text = ''\n",
    "                                for key, value in Headings.items():\n",
    "                                    chunk_text+=value + '\\n'\n",
    "                                all_chunks,metadata = emit_chunk(all_chunks, metadata, chunk_text)\n",
    "                            else:\n",
    "                                metadata['sub_headings'] = {}\n",
    "                                for key, value in Headings.items():\n",
    "                                    metadata['sub_headings']['level_'+str(key)+'_heading'] = value\n",
    "\n",
    "                        text = element.get_text().strip('\\n').strip(' ')\n",
    "                        processed_chunk.append(text)\n",
    "                        count_of_words += len(text.split())\n",
    "                        prev_content = True\n",
    "                        prev_heading = False\n",
    "\n",
    "                elif isinstance(element, TableBlock):\n",
    "                    if prev_heading:\n",
    "                        if len(Headings) > 3:\n",
    "                            chunk_text = ''\n",
    "                            for key, value in Headings.items():\n",
    "                                chunk_text+=value + '\\n'\n",
    "                            all_chunks,metadata = emit_chunk(all_chunks, metadata, chunk_text)\n",
    "                        else:\n",
    "                            metadata['sub_headings'] = {}\n",
    "                            for key, value in Headings.items():\n",
    "                                metadata['sub_headings']['level_'+str(key)+'_heading'] = value\n",
    "                    \n",
    "                    chunk_text = ''\n",
    "                    if prev_content:\n",
    "                        text = '\\n'.join(processed_chunk)\n",
    "                        if len(text.split()) < 20 and len(text.split()) > 5:\n",
    "                            chunk_text = text\n",
    "                            processed_chunk = []\n",
    "\n",
    "                    chunk_text += element.get_text()\n",
    "                    metadata['is_table'] = True\n",
    "                    all_chunks,metadata = emit_chunk(all_chunks, metadata, chunk_text)\n",
    "                    prev_content = True\n",
    "                    prev_heading = False\n",
    "\n",
    "        if processed_chunk:\n",
    "            count_of_words = 0\n",
    "            chunk_text = '\\n'.join(processed_chunk)\n",
    "            if len(chunk_text.split()) > 20:\n",
    "                all_chunks,metadata = emit_chunk(all_chunks, metadata, chunk_text)\n",
    "            processed_chunk = []\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "def get_chunks(filing_date: str,ticker:str, company: str, cik: int, accession_no:str) -> list:\n",
    "    filing = Filing(\n",
    "            form='10-K',\n",
    "            filing_date= filing_date,\n",
    "            company= ticker,\n",
    "            cik=cik,\n",
    "            accession_no= accession_no\n",
    "        )\n",
    "\n",
    "    obj = filing.obj()\n",
    "    chunk_obj = obj.chunked_document\n",
    "\n",
    "    table_of_contents = get_table_of_contents(chunk_obj)\n",
    "    item_heading_dictionary = get_heading_dict(sorted(chunk_obj.list_items()), table_of_contents)\n",
    "    all_chunks = chunk_document(chunk_obj, filing_date, company, item_heading_dictionary,accession_no, form_type = '10-K')\n",
    "    \n",
    "    print(f\"Processed {ticker}, {filing_date}\")\n",
    "    return all_chunks\n",
    "\n",
    "def main():\n",
    "    global TOTAL_CHUNKS_TENK\n",
    "    for (cik, ticker),company in zip(TICKERS.items(),COMPANY_NAMES):\n",
    "        meta_df_path = os.path.join(BASE_PATH_TENK, ticker, '10-K.csv')\n",
    "        meta_df = pd.read_csv(meta_df_path)\n",
    "        meta_df = meta_df[pd.to_datetime(meta_df['filing_date']).dt.year > 2023]\n",
    "\n",
    "        for _, row in meta_df.iterrows():\n",
    "            filing_date = row['filing_date']\n",
    "            accession_no = row['accession_number']\n",
    "            # Convert date to standard format\n",
    "            datetime_obj  = parse(filing_date)\n",
    "            filing_date = datetime_obj.strftime(OUTPUT_FORMAT)\n",
    "            \n",
    "            all_chunks = get_chunks(filing_date, ticker, company, cik, accession_no)\n",
    "            TOTAL_CHUNKS_TENK+=len(all_chunks)\n",
    "            all_chunks_processed = post_processing_chunks(all_chunks)\n",
    "            \n",
    "            chunk_det = {\n",
    "                'filing_date' : filing_date,\n",
    "                'ticker' : ticker,\n",
    "                'file_chunks' : all_chunks_processed\n",
    "            }\n",
    "            \n",
    "            TENK_CHUNKS.append(chunk_det)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65146c27",
   "metadata": {},
   "source": [
    "### SAVE 10K Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "378eca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Use 'w' mode for writing text\n",
    "with open(SAVE_PATH_TENK, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(TENK_CHUNKS, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf07ca3e",
   "metadata": {},
   "source": [
    "## 10-Q CHUNKING\n",
    "\n",
    "### Algorithm - \n",
    "1. Iterate over Parts one by one.\n",
    "2. Check in headings and tables for 'Item' headings and match with the Parts' items.\n",
    "3. Initialize a Dynamic meta data dictionary for every chunk with a 'part_header' : '{part_no, headings_text} and 'parent_header' : '{item_no, heading_text}', 'form_type':'10_K', 'Company':{company name}, 'filing_date':'{date}'\n",
    "3. If any element is a header, mention '--- heading' in front of that and join it back with the parent chunk.\n",
    "4. Every chunk should contain maximum 250 words.\n",
    "5. Every table should be a separate chunk.\n",
    "6. Mention table in the METADATA of table text and keep it a standalone text, no matter what the size is.\n",
    "\n",
    "### Cleaning Chunks.\n",
    "1. Remove the footers using this pattern '.* |.* |[0-9]*'\n",
    "2. Remove the text if they only contain a single number '[0-9]+', it will remove page numbers.\n",
    "3. Remove any text which contains '[a-z]* inc.' and has less than 3 words, lowercase the text for this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c2aff",
   "metadata": {},
   "source": [
    "#### NOTE: NETFLIX 10-Q files does not contain \"PART I\", that is not handled in current code. It has to be handled in next version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ae77f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AAPL, 2025-08-01\n",
      "Processed AAPL, 2025-05-02\n",
      "Processed AAPL, 2025-01-31\n",
      "Processed AAPL, 2024-08-02\n",
      "Processed AAPL, 2024-05-03\n",
      "Processed AAPL, 2024-02-02\n",
      "Processed MSFT, 2025-10-29\n",
      "Processed MSFT, 2025-04-30\n",
      "Processed MSFT, 2025-01-29\n",
      "Processed MSFT, 2024-10-30\n",
      "Processed MSFT, 2024-04-25\n",
      "Processed MSFT, 2024-01-30\n",
      "Processed GOOGL, 2025-10-30\n",
      "Processed GOOGL, 2025-07-24\n",
      "Processed GOOGL, 2025-04-25\n",
      "Processed GOOGL, 2024-10-30\n",
      "Processed GOOGL, 2024-07-24\n",
      "Processed GOOGL, 2024-04-26\n",
      "Processed AMZN, 2025-10-31\n",
      "Processed AMZN, 2025-08-01\n",
      "Processed AMZN, 2025-05-02\n",
      "Processed AMZN, 2024-11-01\n",
      "Processed AMZN, 2024-08-02\n",
      "Processed AMZN, 2024-05-01\n",
      "Processed META, 2025-10-30\n",
      "Processed META, 2025-07-31\n",
      "Processed META, 2025-05-01\n",
      "Processed META, 2024-10-31\n",
      "Processed META, 2024-08-01\n",
      "Processed META, 2024-04-25\n",
      "Processed NVDA, 2025-11-19\n",
      "Processed NVDA, 2025-08-27\n",
      "Processed NVDA, 2025-05-28\n",
      "Processed NVDA, 2024-11-20\n",
      "Processed NVDA, 2024-08-28\n",
      "Processed NVDA, 2024-05-29\n",
      "Processed TSLA, 2025-10-23\n",
      "Processed TSLA, 2025-07-24\n",
      "Processed TSLA, 2025-04-23\n",
      "Processed TSLA, 2024-10-24\n",
      "Processed TSLA, 2024-07-24\n",
      "Processed TSLA, 2024-04-24\n",
      "Processed ORCL, 2025-09-10\n",
      "Processed ORCL, 2025-03-11\n",
      "Processed ORCL, 2024-12-10\n",
      "Processed ORCL, 2024-09-10\n",
      "Processed ORCL, 2024-03-12\n",
      "Processed CRM, 2025-12-04\n",
      "Processed CRM, 2025-09-04\n",
      "Processed CRM, 2025-05-29\n",
      "Processed CRM, 2024-12-04\n",
      "Processed CRM, 2024-08-29\n",
      "Processed CRM, 2024-05-30\n",
      "Processed NFLX, 2025-10-22\n",
      "Processed NFLX, 2025-07-18\n",
      "Processed NFLX, 2025-04-18\n",
      "Processed NFLX, 2024-10-18\n",
      "Processed NFLX, 2024-07-19\n",
      "Processed NFLX, 2024-04-22\n",
      "Processed ADBE, 2025-09-24\n",
      "Processed ADBE, 2025-06-25\n",
      "Processed ADBE, 2025-03-26\n",
      "Processed ADBE, 2024-09-25\n",
      "Processed ADBE, 2024-06-26\n",
      "Processed ADBE, 2024-03-27\n"
     ]
    }
   ],
   "source": [
    "metadata_csv = pd.DataFrame()\n",
    "\n",
    "OUTPUT_FORMAT = \"%Y-%m-%d\"\n",
    "\n",
    "COMPANY_NAMES = [\n",
    "        \"Apple Inc.\",\n",
    "        \"MICROSOFT CORP\",\n",
    "        \"Alphabet Inc.\",\n",
    "        \"AMAZON COM INC.\",\n",
    "        \"Meta Platforms, Inc.\",\n",
    "        \"NVIDIA CORP\",\n",
    "        \"Tesla, Inc.\",\n",
    "        \"ORACLE CORP\",\n",
    "        \"Salesforce, Inc.\",\n",
    "        \"NETFLIX INC\",\n",
    "        \"ADOBE INC.\"\n",
    "    ]\n",
    "\n",
    "TICKERS = {\n",
    "        320193 : \"AAPL\",\n",
    "        789019: \"MSFT\",\n",
    "        1652044: \"GOOGL\",\n",
    "        1018724: \"AMZN\",\n",
    "        1326801 : \"META\",\n",
    "        1045810: \"NVDA\",\n",
    "        1318605 : \"TSLA\",\n",
    "        1341439 : \"ORCL\",\n",
    "        1108524 : \"CRM\",\n",
    "        1065280 : \"NFLX\",\n",
    "        796343 : \"ADBE\"\n",
    "    }\n",
    "\n",
    "#Clean Text\n",
    "def clean_abnormal_text(text):\n",
    "    # 1. Remove long repetitive sequences (3+ dashes, dots, or symbols)\n",
    "    # This cleans up the \"----\" and \"......\" patterns common in broken tables\n",
    "    text = re.sub(r'[-._*~=]{3,}', ' ', text)\n",
    "    \n",
    "    # 2. Remove \"DAU/MAU\" noise blocks\n",
    "    # Specifically targets \"DAU/MAU:\" followed by numbers and percentages\n",
    "    text = re.sub(r'DAU/MAU:[\\d% \\-]+', ' ', text)\n",
    "    \n",
    "    # 3. Remove excessive whitespace\n",
    "    # Keep standard spaces but remove non-standard ones\n",
    "    text = re.sub(r'\\s+', ' ', text) # Normalize all whitespace to single spaces\n",
    "    \n",
    "    # 4. Remove isolated clusters of special characters\n",
    "    # This removes things like \"- - - \" that linger after other steps\n",
    "    text = re.sub(r'\\s([^\\w\\s])\\s', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def check_and_restore(gen):\n",
    "    try:\n",
    "        first_item = next(gen)\n",
    "        # Re-construct the generator by prepending the item we just took\n",
    "        return False, chain([first_item], gen)\n",
    "    except StopIteration:\n",
    "        return True, gen\n",
    "\n",
    "\n",
    "#Post processing the exisiting chunks\n",
    "def post_processing_chunks(all_chunks):\n",
    "    global CHUNK_COUNTER\n",
    "    all_chunks_processed = all_chunks.copy()\n",
    "    splitter = SentenceSplitter(\n",
    "                chunk_size=300,\n",
    "                chunk_overlap=60\n",
    "            )\n",
    "    for chunk in all_chunks_processed:\n",
    "        chunk['ID'] = CHUNK_COUNTER\n",
    "        CHUNK_COUNTER+=1\n",
    "        if chunk[\"Metadata\"]:\n",
    "            text = chunk[\"Chunks\"][\"parent_chunk\"]\n",
    "            chunk[\"Chunks\"]['child_chunks'] = {}\n",
    "\n",
    "            if chunk[\"Metadata\"]['is_table']:\n",
    "                chunk[\"Chunks\"]['child_chunks'][f\"{chunk['ID']}_{0}\"] = text\n",
    "                continue\n",
    "            \n",
    "            text  = clean_abnormal_text(text)\n",
    "            documents = [Document(text = text)]\n",
    "            nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "            for i,node in enumerate(nodes):\n",
    "                child_chunk_dict = chunk[\"Chunks\"]['child_chunks']\n",
    "                if i==0:\n",
    "                    child_chunk_dict[f\"{chunk['ID']}_{i}\"]=node.text\n",
    "                elif i>0:\n",
    "                    if len(node.text.split()) > 50:\n",
    "                        child_chunk_dict[f\"{chunk['ID']}_{i}\"]=node.text\n",
    "                    elif node.text not in child_chunk_dict[list(child_chunk_dict)[-1]]:\n",
    "                        child_chunk_dict[list(child_chunk_dict)[-1]]+=node.text\n",
    "                        \n",
    "    return all_chunks_processed\n",
    "\n",
    "#We will is_table as true if 30% of all_items matches one of the tables\n",
    "def get_table_of_contents(chunk_obj):\n",
    "    def is_table(table,all_items):\n",
    "        q_items = all_items[:math.ceil(len(all_items)*0.3)]\n",
    "        table = table.get_text().lower()\n",
    "        for item in q_items:\n",
    "            if item.lower() not in table:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    all_items = chunk_obj.list_items()\n",
    "    table_of_contents = pd.DataFrame()\n",
    "    MAX_ITEMS = -1\n",
    "    ITEM_COLUMN_INDEX = -1\n",
    "    for table in chunk_obj.tables():\n",
    "        if is_table(table, all_items):\n",
    "            table_of_contents = table.to_dataframe()\n",
    "            \n",
    "            #Naming the 'Items' columns and 'Headings' columns\n",
    "            for i in range(table_of_contents.shape[1]):\n",
    "                NUM_ITEMS = sum('item' in item.lower() for item in table_of_contents.iloc[:,i])\n",
    "                if NUM_ITEMS > MAX_ITEMS:\n",
    "                    ITEM_COLUMN_INDEX = i\n",
    "                    MAX_ITEMS = NUM_ITEMS\n",
    "            \n",
    "            #Naming the columns\n",
    "            columns = [''] * table_of_contents.shape[1]\n",
    "            columns[ITEM_COLUMN_INDEX] = 'Index'\n",
    "            columns[ITEM_COLUMN_INDEX + 1] = 'Headings'\n",
    "\n",
    "            #Setting the columns to datarame's column attribute\n",
    "            table_of_contents.columns = columns\n",
    "            \n",
    "            return table_of_contents\n",
    "\n",
    "def get_correct_item(item, all_items):\n",
    "    for actual_item in all_items:\n",
    "        if re.fullmatch(item+'[^a-zA-Z0-9]*',actual_item):\n",
    "            return actual_item\n",
    "    \n",
    "    item = item.strip('. ').split()\n",
    "    item = item[0].capitalize() + ' ' + item[1]\n",
    "    return item\n",
    "\n",
    "\n",
    "\n",
    "def get_heading_dict(all_items, table_of_contents):\n",
    "    #Dictionary of items\n",
    "    headings = {}\n",
    "    part_1 = False\n",
    "    part_2 = False\n",
    "\n",
    "    #if table of contents is none, then we will return items themselves in keys and value both\n",
    "    if table_of_contents.empty :\n",
    "        headings['Part I'] = {'Heading':'Financial Information','Items':[]}\n",
    "        headings['Part II'] = {'Heading':'Other Information','Items':[]}\n",
    "        return headings\n",
    "\n",
    "    for _, row in table_of_contents.iterrows() :\n",
    "        if re.fullmatch(r'part i[^a-zA-Z0-9]*', row['Index'].lower()) or 'financial information' in row['Index'].lower() and not part_1:\n",
    "            headings['Part I'] = {'Heading':'Part I: Financial Information','Items':{}}\n",
    "            part_1, part_2 = True, False\n",
    "            \n",
    "        elif re.fullmatch(r'part i[^a-zA-Z0-9]*', row['Headings'].lower()) or 'financial information' in row['Headings'].lower() and not part_1:\n",
    "            headings['Part I'] = {'Heading':'Part I: Financial Information','Items':{}}\n",
    "            part_1, part_2 = True, False\n",
    "\n",
    "        elif re.fullmatch(r'part ii[^a-zA-Z0-9]*', row['Index'].lower()) or 'other information' in row['Index'].lower() and not part_2:\n",
    "            headings['Part II'] = {'Heading':'Part II: Other Information','Items':{}}\n",
    "            part_1, part_2 = False, True\n",
    "\n",
    "        elif re.fullmatch(r'part ii[^a-zA-Z0-9]*', row['Headings'].lower()) or 'other information' in row['Headings'].lower() and not part_2:\n",
    "            headings['Part II'] = {'Heading':'Part II: Other Information','Items':{}}\n",
    "            part_1, part_2 = False, True\n",
    "\n",
    "        if part_1:\n",
    "            item = row['Index'].strip('.')\n",
    "            heading = row['Headings']\n",
    "            if 'item' in item.lower():\n",
    "                actual_item = get_correct_item(item,all_items)\n",
    "                headings['Part I']['Items'][actual_item] = actual_item + ': '+ heading.strip(':. \\n')\n",
    "\n",
    "        elif part_2:\n",
    "            item = row['Index'].strip('.')\n",
    "            heading = row['Headings']\n",
    "            if 'item' in item.lower():\n",
    "                actual_item = get_correct_item(item,all_items)\n",
    "                headings['Part II']['Items'][actual_item] = actual_item + ': '+ heading.strip(':. \\n')\n",
    "\n",
    "    return headings\n",
    "    \n",
    "#Create chunk function\n",
    "def emit_chunk(all_chunks, metadata, text):\n",
    "    if len(text.split()) > 20:\n",
    "        all_chunks.append({\n",
    "            \"Metadata\": deepcopy(metadata),\n",
    "            \"Chunks\": {\"parent_chunk\": text}\n",
    "        })\n",
    "    metadata.pop(\"sub_headings\", None)\n",
    "    metadata[\"is_table\"] = False\n",
    "    return all_chunks, metadata\n",
    "\n",
    "def chunk_document(chunk_obj, filing_date, company, heading_dictionary,accession_no, form_type = '10-Q'):\n",
    "\n",
    "    regex1 = re.compile(\n",
    "                        r\"\"\"\n",
    "                            \\n*                     # optional newlines at start\n",
    "                            .*?                     # company name\n",
    "                            \\s\\|\\s                  # ' | '\n",
    "                            .*?                     # Any text like 'Q2'\n",
    "                            \\d{4}                   # 4-digit year\n",
    "                            \\sForm\\s10-Q            # ' Form 10-K'\n",
    "                            \\s\\|\\s                  # ' | '\n",
    "                            \\d+                     # page number\n",
    "                            \\n*                     # optional newlines at end\n",
    "                            $\"\"\",\n",
    "                        re.VERBOSE,\n",
    "                        )\n",
    "\n",
    "    # Regex 2: Matches only digits\n",
    "    # ^\\d+$\n",
    "    regex2 = re.compile(r\"^\\n*\\d+\\n*$\")\n",
    "\n",
    "    metadata = {'part_heading':None,'item_heading':None, 'filing_date':filing_date, 'company':company, 'form':form_type, 'is_table':False,\"accession_no\":accession_no}\n",
    "    #Get all the items present in the table\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    processed_chunk = []\n",
    "    prev_heading = {'T/F':False,'Heading': None}\n",
    "    count_of_words = 0\n",
    "    \n",
    "    for part in ['Part I','Part II']:\n",
    "        metadata['part_heading'] = heading_dictionary[part]['Heading']\n",
    "        part_items = list(heading_dictionary[part]['Items'].keys())\n",
    "       \n",
    "        chunk_for_part = chunk_obj.chunks_for_part(part)\n",
    "        \n",
    "        Headings = {}\n",
    "        prev_content = False\n",
    "        prev_heading = False\n",
    "        count_of_words = 0\n",
    "        \n",
    "        for chunk in chunk_for_part:\n",
    "            for element in chunk:\n",
    "                #We want only text or tables\n",
    "                if isinstance(element, (TextBlock)):\n",
    "                    if element.is_header:\n",
    "                        #If header contains any of the following should be discarded\n",
    "                        if regex1.fullmatch(element.get_text()) or regex2.fullmatch(element.get_text()) or 'table of content' in element.get_text().lower():\n",
    "                            continue\n",
    "\n",
    "                        #Extracting Items                        \n",
    "                        for item in part_items:\n",
    "                            item_heading = re.sub(\"item\\s*\\d*\\w*:\\s*\",\"\",heading_dictionary[part]['Items'][item].lower())\n",
    "                            if item_heading in element.get_text().lower():\n",
    "                                #Clearing of any existing chunk\n",
    "                                if processed_chunk:\n",
    "                                    count_of_words = 0\n",
    "                                    chunk_text = '\\n'.join(processed_chunk)\n",
    "                                    if len(chunk_text.split()) > 20:\n",
    "                                        all_chunks, metadata = emit_chunk(all_chunks, metadata, chunk_text)\n",
    "                                    processed_chunk = []\n",
    "                                #Updating the new item heading\n",
    "                                metadata['item_heading'] = heading_dictionary[part]['Items'][item]\n",
    "                        \n",
    "                        #Now retrieving text for processing\n",
    "                        text = element.get_text().strip('\\n').strip(' ')\n",
    "\n",
    "                        #If text contains 'item' or 'part' it should be discarded.\n",
    "                        #As it has been already taken care of\n",
    "                        if 'item' in text.lower() or 'part' in text.lower():\n",
    "                            continue\n",
    "                        \n",
    "                        #If previous element was content then processed chunk has to be added or flushed\n",
    "                        if prev_content and processed_chunk:\n",
    "                            chunk_text = ' '.join(processed_chunk)\n",
    "                            count_of_words = 0\n",
    "                            if len(chunk_text.split()) > 20:\n",
    "                                all_chunks, metadata = emit_chunk(all_chunks, metadata, chunk_text)\n",
    "                            processed_chunk = []\n",
    "                            Headings = {}\n",
    "                        \n",
    "                        #Appending the next heading in existing dict\n",
    "                        if Headings:\n",
    "                            max_level = max(list(Headings.keys()))\n",
    "                            Headings[max_level+1] = text\n",
    "                        #Creating a new dictionary\n",
    "                        else:\n",
    "                            Headings[1] = text\n",
    "                        \n",
    "                        #Setting previous content to False and preious heading to True\n",
    "                        prev_content = False\n",
    "                        prev_heading = True\n",
    "                    \n",
    "                    #If the element is not header\n",
    "                    else:\n",
    "                        if prev_heading:\n",
    "                            #If length of heading is \n",
    "                            if len(Headings) > 3:\n",
    "                                chunk_text = ''\n",
    "                                for key, value in Headings.items():\n",
    "                                    chunk_text+=value + '\\n'\n",
    "                                all_chunks, metadata = emit_chunk(all_chunks, metadata, chunk_text)\n",
    "\n",
    "                            #If length is not more than 3 create sub headings dictionary\n",
    "                            else:\n",
    "                                metadata['sub_headings'] = {}\n",
    "                                for key, value in Headings.items():\n",
    "                                    metadata['sub_headings']['level_'+str(key)+'_heading'] = value\n",
    "\n",
    "                        text = element.get_text().strip('\\n').strip(' ')\n",
    "                        processed_chunk.append(text)\n",
    "                        count_of_words += len(text.split())\n",
    "                        prev_content = True\n",
    "                        prev_heading = False\n",
    "\n",
    "                elif isinstance(element, TableBlock):\n",
    "                    #Update Metadata parent_heading key\n",
    "                    for item in part_items:\n",
    "                        item_heading = re.sub(\"item\\s*\\d*\\w*:\\s*\",\"\",heading_dictionary[part]['Items'][item].lower())\n",
    "                        if item_heading in element.get_text().lower():\n",
    "                            if processed_chunk:\n",
    "                                chunk_text = '\\n'.join(processed_chunk)\n",
    "                                if len(chunk_text.split()) > 20:\n",
    "                                    all_chunks, metadata = emit_chunk(all_chunks, metadata, chunk_text)\n",
    "                                processed_chunk = []\n",
    "                                count_of_words = 0\n",
    "                            metadata['item_heading'] = heading_dictionary[part]['Items'][item]\n",
    "\n",
    "                    #Eliminating the table of contents by checking for number of 'item' in text\n",
    "                    if element.get_text().lower().count('item') < 6:\n",
    "                        if prev_heading or prev_content:\n",
    "                            if len(Headings) > 3:\n",
    "                                chunk_text = ''\n",
    "                                for key, value in Headings.items():\n",
    "                                    chunk_text+=value + '\\n'\n",
    "                                all_chunks, metadata = emit_chunk(all_chunks, metadata, chunk_text)\n",
    "                            else:\n",
    "                                metadata['sub_headings'] = {}\n",
    "                                for key, value in Headings.items():\n",
    "                                    metadata['sub_headings']['level_'+str(key)+'_heading'] = value\n",
    "\n",
    "                        chunk_text = ''\n",
    "                        if prev_content:\n",
    "                            text = '\\n'.join(processed_chunk)\n",
    "                            if len(text.split()) < 20 and len(text.split()) > 5:\n",
    "                                chunk_text = text\n",
    "                                processed_chunk = []\n",
    "\n",
    "                        chunk_text += element.get_text()\n",
    "                        \n",
    "                        metadata['is_table'] = True\n",
    "                        all_chunks, metadata = emit_chunk(all_chunks, metadata, chunk_text)\n",
    "                        prev_content = True\n",
    "                        prev_heading = False\n",
    "\n",
    "        if processed_chunk:\n",
    "            count_of_words = 0\n",
    "            chunk_text = '\\n'.join(processed_chunk)\n",
    "            if len(chunk_text.split()) > 20:\n",
    "                all_chunks, metadata = emit_chunk(all_chunks, metadata, chunk_text)\n",
    "            processed_chunk = []\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "def get_chunks(filing_date: str,ticker:str, company: str, cik: int, accession_no:str) -> list:\n",
    "    # print(filing_date, ticker, cik, accession_no)\n",
    "    filing = Filing(\n",
    "            form='10-Q',\n",
    "            filing_date= filing_date,\n",
    "            company= ticker,\n",
    "            cik=cik,\n",
    "            accession_no= accession_no\n",
    "        )\n",
    "\n",
    "    obj = filing.obj()\n",
    "    chunk_obj = obj.chunked_document\n",
    "\n",
    "    table_of_contents = get_table_of_contents(chunk_obj)\n",
    "    heading_dictionary = get_heading_dict(sorted(chunk_obj.list_items()),table_of_contents)\n",
    "    all_chunks = chunk_document(chunk_obj, filing_date, company, heading_dictionary, accession_no, form_type = '10-Q')\n",
    "    \n",
    "    print(f\"Processed {ticker}, {filing_date}\")\n",
    "    return all_chunks\n",
    "\n",
    "def main():\n",
    "    global TOTAL_CHUNKS_TENQ\n",
    "    for (cik, ticker),company in zip(TICKERS.items(),COMPANY_NAMES):\n",
    "        meta_df_path = os.path.join(BASE_PATH_TENQ, ticker, '10-Q.csv')\n",
    "        meta_df = pd.read_csv(meta_df_path)\n",
    "        meta_df = meta_df[pd.to_datetime(meta_df['filing_date']).dt.year > 2023]\n",
    "        for _, row in meta_df.iterrows():\n",
    "            filing_date = row['filing_date']\n",
    "            accession_no = row['accession_number']\n",
    "\n",
    "            # Convert date to standard format\n",
    "            datetime_obj  = parse(filing_date)\n",
    "            filing_date = datetime_obj.strftime(OUTPUT_FORMAT)\n",
    "            \n",
    "            all_chunks = get_chunks(filing_date, ticker, company, cik, accession_no)\n",
    "            TOTAL_CHUNKS_TENQ+=len(all_chunks)\n",
    "            all_chunks_processed = post_processing_chunks(all_chunks)\n",
    "            \n",
    "            chunk_det = {\n",
    "                'filing_date' : filing_date,\n",
    "                'ticker' : ticker,\n",
    "                'file_chunks' : all_chunks_processed\n",
    "            }\n",
    "            TENQ_CHUNKS.append(chunk_det)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473bcf1f",
   "metadata": {},
   "source": [
    "### SAVE 10-Q CHUNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f58f653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Use 'w' mode for writing text\n",
    "with open(SAVE_PATH_TENQ, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(TENQ_CHUNKS, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9b420",
   "metadata": {},
   "source": [
    "## Save all Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "128f3783",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenk_tenq_chunks = TENK_CHUNKS + TENQ_CHUNKS\n",
    "\n",
    "with open(SAVE_PATH_ALL,'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(tenk_tenq_chunks, f, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dc1590",
   "metadata": {},
   "source": [
    "## Saving all transcripts into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba4b2cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "CHUNK_ID = os.getenv(\"LATEST_CHUNK_ID\")\n",
    "CHUNK_ID = int(CHUNK_ID)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a959106c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed for AAPL and 2024Q1\n",
      "Processed for AAPL and 2024Q2\n",
      "Processed for AAPL and 2024Q3\n",
      "Processed for AAPL and 2025Q1\n",
      "Processed for AAPL and 2025Q2\n",
      "Processed for AAPL and 2025Q3\n",
      "Processed for MSFT and 2024Q1\n",
      "Processed for MSFT and 2024Q2\n",
      "Processed for MSFT and 2024Q3\n",
      "Processed for MSFT and 2025Q1\n",
      "Processed for MSFT and 2025Q2\n",
      "Processed for MSFT and 2025Q3\n",
      "Processed for GOOGL and 2024Q1\n",
      "Processed for GOOGL and 2024Q2\n",
      "Processed for GOOGL and 2024Q3\n",
      "Processed for GOOGL and 2025Q1\n",
      "Processed for GOOGL and 2025Q2\n",
      "Processed for GOOGL and 2025Q3\n",
      "Processed for AMZN and 2024Q1\n",
      "Processed for AMZN and 2024Q2\n",
      "Processed for AMZN and 2024Q3\n",
      "Processed for AMZN and 2025Q1\n",
      "Processed for AMZN and 2025Q2\n",
      "Processed for AMZN and 2025Q3\n",
      "Processed for META and 2024Q1\n",
      "Processed for META and 2024Q2\n",
      "Processed for META and 2024Q3\n",
      "Processed for META and 2025Q1\n",
      "Processed for META and 2025Q2\n",
      "Processed for META and 2025Q3\n",
      "Processed for NVDA and 2024Q1\n",
      "Processed for NVDA and 2024Q2\n",
      "Processed for NVDA and 2024Q3\n",
      "Processed for NVDA and 2025Q1\n",
      "Processed for NVDA and 2025Q2\n",
      "Processed for NVDA and 2025Q3\n",
      "Processed for TSLA and 2024Q1\n",
      "Processed for TSLA and 2024Q2\n",
      "Processed for TSLA and 2024Q3\n",
      "Processed for TSLA and 2025Q1\n",
      "Processed for TSLA and 2025Q2\n",
      "Processed for TSLA and 2025Q3\n",
      "Processed for ORCL and 2024Q1\n",
      "Processed for ORCL and 2024Q2\n",
      "Processed for ORCL and 2024Q3\n",
      "Processed for ORCL and 2025Q1\n",
      "Processed for ORCL and 2025Q2\n",
      "Processed for ORCL and 2025Q3\n",
      "Processed for CRM and 2024Q1\n",
      "Processed for CRM and 2024Q2\n",
      "Processed for CRM and 2024Q3\n",
      "Processed for CRM and 2025Q1\n",
      "Processed for CRM and 2025Q2\n",
      "Processed for CRM and 2025Q3\n",
      "Processed for NFLX and 2024Q1\n",
      "Processed for NFLX and 2024Q2\n",
      "Processed for NFLX and 2024Q3\n",
      "Processed for NFLX and 2025Q1\n",
      "Processed for NFLX and 2025Q2\n",
      "Processed for NFLX and 2025Q3\n",
      "Processed for ADBE and 2024Q1\n",
      "Processed for ADBE and 2024Q2\n",
      "Processed for ADBE and 2024Q3\n",
      "Processed for ADBE and 2025Q1\n",
      "Processed for ADBE and 2025Q2\n",
      "Processed for ADBE and 2025Q3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TICKERS = {\n",
    "    \"AAPL\": \"Apple Inc.\",\n",
    "    \"MSFT\": \"MICROSOFT CORP\",\n",
    "    \"GOOGL\": \"Alphabet Inc.\",\n",
    "    \"AMZN\": \"AMAZON COM INC.\",\n",
    "    \"META\": \"Meta Platforms, Inc.\",\n",
    "    \"NVDA\": \"NVIDIA CORP\",\n",
    "    \"TSLA\": \"Tesla, Inc.\",\n",
    "    \"ORCL\": \"ORACLE CORP\",\n",
    "    \"CRM\": \"Salesforce, Inc.\",\n",
    "    \"NFLX\": \"NETFLIX INC\",\n",
    "    \"ADBE\": \"ADOBE INC.\"\n",
    "}\n",
    "years = [\"2024\",\"2025\"]\n",
    "quarters = ['Q1','Q2','Q3']\n",
    "\n",
    "base_path = \"C:/Users/paymo/Downloads/QuantiGence/data/metadata/10-Q/{ticker}/earnings_transcript_{year}{quarter}.json\"\n",
    "save_path = \"C:/Users/paymo/Downloads/QuantiGence/Chunks/all_transcripts.json\"\n",
    "all_transcripts = []\n",
    "\n",
    "#Post processing the exisiting chunks\n",
    "def post_processing_chunks(all_chunks,company,year,quarter):\n",
    "    global CHUNK_ID\n",
    "    all_chunks_copy = all_chunks.copy()\n",
    "    splitter = SentenceSplitter(\n",
    "                chunk_size=300,\n",
    "                chunk_overlap=60\n",
    "            )\n",
    "    all_chunks_processed = []\n",
    "    for chunk in all_chunks_copy[\"transcript\"]:\n",
    "        metadata = {}\n",
    "        metadata[\"speaker\"] = chunk[\"speaker\"]\n",
    "        metadata[\"title\"] = chunk[\"title\"]\n",
    "        metadata[\"company\"] = company\n",
    "        metadata[\"year\"] = year\n",
    "        metadata[\"quarter\"] = quarter\n",
    "\n",
    "        parent_chunk = {}\n",
    "        parent_chunk[\"ID\"] = CHUNK_ID\n",
    "        parent_chunk[\"chunk\"] = chunk[\"content\"]\n",
    "\n",
    "        child_chunks = {}\n",
    "        if metadata:\n",
    "            text = parent_chunk[\"chunk\"]\n",
    "\n",
    "            documents = [Document(text = text)]\n",
    "            nodes = splitter.get_nodes_from_documents(documents)\n",
    "            for i,node in enumerate(nodes):\n",
    "                if i==0:\n",
    "                    child_chunks[f\"{parent_chunk['ID']}_{i}\"]=node.text\n",
    "                elif i>0:\n",
    "                    if len(node.text.split()) > 50:\n",
    "                        child_chunks[f\"{parent_chunk['ID']}_{i}\"]=node.text\n",
    "                    elif node.text not in child_chunks[list(child_chunks)[-1]]:\n",
    "                        child_chunks[list(child_chunks)[-1]]+=node.text\n",
    "\n",
    "        CHUNK_ID+=1\n",
    "        all_chunks_processed.append({\"Metadata\":metadata,\"ParentChunk\":parent_chunk,\"ChildChunks\":child_chunks}) \n",
    "        \n",
    "    return all_chunks_processed\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    for ticker, company in TICKERS.items():\n",
    "        for year in years:\n",
    "            for quarter in quarters:\n",
    "                curr_path = base_path.format(ticker=ticker,year=year,quarter=quarter)\n",
    "                data = None\n",
    "                with open(curr_path,'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                all_chunks_processed = post_processing_chunks(data,company,year,quarter)\n",
    "                \n",
    "                transcript = {\n",
    "                    \"ticker\":ticker,\n",
    "                    \"quarter\":quarter,\n",
    "                    \"year\":year,\n",
    "                    \"transcript\":all_chunks_processed\n",
    "                }\n",
    "                all_transcripts.append(transcript)\n",
    "                print(f\"Processed for {ticker} and {year+quarter}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfc1ecf",
   "metadata": {},
   "source": [
    "## SAVE ALL TRANSCRIPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a9b1824",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path,'w',encoding=\"utf-8\") as file:\n",
    "    json.dump(all_transcripts,file,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21624519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16014"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHUNK_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622bb275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantigence-oNV_nXK--py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
